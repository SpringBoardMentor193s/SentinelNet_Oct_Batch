{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ac328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2bfc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names\n",
    "column_names = [\n",
    "    'duration', 'protocol_type', 'service', 'flag',\n",
    "    'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot',\n",
    "    'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted',\n",
    "    'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds',\n",
    "    'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n",
    "    'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate',\n",
    "    'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
    "    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
    "    'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'class', 'difficulty'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3326db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# Load the training data\n",
    "train_df = pd.read_csv(\n",
    "    r\"C:\\Users\\yasha\\Downloads\\nsl-kdd\\KDDTrain+.txt\",\n",
    "    header=None,\n",
    "    names=column_names\n",
    ")\n",
    "\n",
    "# Drop the 'difficulty' column\n",
    "train_df.drop(['difficulty'], axis=1, inplace=True)\n",
    "\n",
    "# Load the testing data\n",
    "test_df = pd.read_csv(\n",
    "    r\"C:\\Users\\yasha\\Downloads\\nsl-kdd\\KDDTest+.txt\",\n",
    "    header=None,\n",
    "    names=column_names\n",
    ")\n",
    "\n",
    "# Drop the 'difficulty' column\n",
    "test_df.drop(['difficulty'], axis=1, inplace=True)\n",
    "\n",
    "# Print basic info\n",
    "print(\"Training Data\")\n",
    "print(f\"Shape: {train_df.shape}\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\n--- Testing Data ---\")\n",
    "print(f\"Shape: {test_df.shape}\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae27831",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c0d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['protocol_type','class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf93258",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a6acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary classification target\n",
    "train_df['attack_binary'] = train_df['class'].apply(lambda x: 0 if x == 'normal' else 1)\n",
    "test_df['attack_binary']  = test_df['class'].apply(lambda x: 0 if x == 'normal' else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8963ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='attack_binary', data=train_df, palette='Set1')\n",
    "plt.title('Binary Attack Distribution')\n",
    "plt.xlabel('0 = Normal, 1 = Attack')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print(train_df['attack_binary'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd06abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(x='protocol_type', hue='attack_binary', data=train_df, palette='Set2')\n",
    "plt.title('Protocol Type vs Attack Binary')\n",
    "plt.show()\n",
    "\n",
    "print(train_df['protocol_type'].value_counts())\n",
    "print('\\n',train_df.groupby(['protocol_type', 'attack_binary']).size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d87fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(x='flag', hue='attack_binary', data=train_df, palette='pastel')\n",
    "plt.title('Connection Flag vs Attack Binary')\n",
    "plt.show()\n",
    "\n",
    "print(train_df['flag'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0036b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_services = train_df['service'].value_counts().nlargest(20).index\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(y='service', data=train_df[train_df['service'].isin(top_services)],\n",
    "              order=top_services, hue='attack_binary', palette='Set3')\n",
    "plt.title('Top 20 Services vs Attack Binary')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Service')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107664e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_df = train_df[train_df['attack_binary'] == 1]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.countplot(y='class', data=attack_df, order=attack_df['class'].value_counts().index, palette='viridis')\n",
    "plt.title(\"Distribution of Different Attack Types\", fontsize=16)\n",
    "plt.xlabel('Count', fontsize=12)\n",
    "plt.ylabel('Attack Type', fontsize=12)\n",
    "plt.xscale('log')  # log scale for better visibility\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148da4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 side-by-side subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 6))\n",
    "\n",
    "# Plot 1: Protocol Type Distribution\n",
    "sns.countplot(ax=axes[0], x='protocol_type', data=train_df, palette='magma')\n",
    "axes[0].set_title('Protocol Type Distribution')\n",
    "\n",
    "# Plot 2: Top 10 Service Types\n",
    "sns.countplot(\n",
    "    ax=axes[1],\n",
    "    x='service',\n",
    "    data=train_df,\n",
    "    order=train_df['service'].value_counts().iloc[:10].index,\n",
    "    palette='plasma'\n",
    ")\n",
    "axes[1].set_title('Top 10 Service Types')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Flag Distribution\n",
    "sns.countplot(\n",
    "    ax=axes[2],\n",
    "    x='flag',\n",
    "    data=train_df,\n",
    "    order=train_df['flag'].value_counts().index,\n",
    "    palette='cividis'\n",
    ")\n",
    "axes[2].set_title('Flag Distribution')\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9de285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X_train_raw = train_df.drop(['class', 'attack_binary'], axis=1)\n",
    "y_train = train_df['attack_binary']\n",
    "\n",
    "X_test_raw = test_df.drop(['class', 'attack_binary'], axis=1)\n",
    "y_test = test_df['attack_binary']\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = ['protocol_type', 'service', 'flag']\n",
    "numerical_cols = X_train_raw.columns.drop(categorical_cols)\n",
    "\n",
    "print(\"Categorical Columns:\", categorical_cols)\n",
    "print(\"Numerical Columns:\", numerical_cols.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4681ff9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encode categorical features\n",
    "X_train_encoded = pd.get_dummies(X_train_raw, columns=categorical_cols, drop_first=True)\n",
    "X_test_encoded = pd.get_dummies(X_test_raw, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Align columns to ensure test set has same features as train set\n",
    "train_cols = X_train_encoded.columns\n",
    "test_cols = X_test_encoded.columns\n",
    "\n",
    "# Add missing columns in test set\n",
    "missing_in_test = set(train_cols) - set(test_cols)\n",
    "for c in missing_in_test:\n",
    "    X_test_encoded[c] = 0\n",
    "\n",
    "# Add missing columns in train set\n",
    "missing_in_train = set(test_cols) - set(train_cols)\n",
    "for c in missing_in_train:\n",
    "    X_train_encoded[c] = 0\n",
    "\n",
    "# To ensure same order in both train and test sets\n",
    "X_test_encoded = X_test_encoded[train_cols]\n",
    "\n",
    "print(f\"Shape of training data after encoding: {X_train_encoded.shape}\")\n",
    "print(f\"Shape of testing data after encoding: {X_test_encoded.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420bfa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69699bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scaled Training Data Head ---\n",
      "   duration  src_bytes  dst_bytes      land  wrong_fragment    urgent  \\\n",
      "0 -0.110249  -0.007679  -0.004919 -0.014089       -0.089486 -0.007736   \n",
      "1 -0.110249  -0.007737  -0.004919 -0.014089       -0.089486 -0.007736   \n",
      "2 -0.110249  -0.007762  -0.004919 -0.014089       -0.089486 -0.007736   \n",
      "3 -0.110249  -0.007723  -0.002891 -0.014089       -0.089486 -0.007736   \n",
      "4 -0.110249  -0.007728  -0.004814 -0.014089       -0.089486 -0.007736   \n",
      "\n",
      "        hot  num_failed_logins  logged_in  num_compromised  ...  flag_REJ  \\\n",
      "0 -0.095076          -0.027023  -0.809262        -0.011664  ...     False   \n",
      "1 -0.095076          -0.027023  -0.809262        -0.011664  ...     False   \n",
      "2 -0.095076          -0.027023  -0.809262        -0.011664  ...     False   \n",
      "3 -0.095076          -0.027023   1.235694        -0.011664  ...     False   \n",
      "4 -0.095076          -0.027023   1.235694        -0.011664  ...     False   \n",
      "\n",
      "   flag_RSTO  flag_RSTOS0  flag_RSTR  flag_S0  flag_S1  flag_S2  flag_S3  \\\n",
      "0      False        False      False    False    False    False    False   \n",
      "1      False        False      False    False    False    False    False   \n",
      "2      False        False      False     True    False    False    False   \n",
      "3      False        False      False    False    False    False    False   \n",
      "4      False        False      False    False    False    False    False   \n",
      "\n",
      "   flag_SF  flag_SH  \n",
      "0     True    False  \n",
      "1     True    False  \n",
      "2    False    False  \n",
      "3     True    False  \n",
      "4     True    False  \n",
      "\n",
      "[5 rows x 119 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Identify the new numerical columns (original ones)\n",
    "numerical_cols_to_scale = numerical_cols  \n",
    "\n",
    "# Fit on training data and transform both train and test data\n",
    "X_train_scaled = X_train_encoded.copy()\n",
    "X_test_scaled = X_test_encoded.copy()\n",
    "\n",
    "X_train_scaled[numerical_cols_to_scale] = StandardScaler().fit_transform(X_train_encoded[numerical_cols_to_scale])\n",
    "X_test_scaled[numerical_cols_to_scale] = StandardScaler().fit(X_train_encoded[numerical_cols_to_scale]).transform(X_test_encoded[numerical_cols_to_scale])\n",
    "\n",
    "print(\"\\n--- Scaled Training Data Head ---\")\n",
    "print(X_train_scaled.head())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
